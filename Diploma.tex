%\usepackage{extsizes}
\documentclass[14pt, a4paper, russian]{extreport}

\linespread{1.3}

\usepackage{titlesec}
\usepackage[centertags]{amsmath}
\usepackage{amsthm,amsfonts,amssymb}
\usepackage{indentfirst}

\usepackage{extsizes}
\usepackage[left=25mm,right=10mm,top=20mm,bottom=20mm,bindingoffset=0cm]{geometry}

\usepackage{cmap}
\usepackage{multirow}
\usepackage{float}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{qtree}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
%\usepackage{pscyr}
\RequirePackage{graphicx}
\RequirePackage{subfig}
\RequirePackage{pgfplots}
\usepackage{thmtools}   
\usepackage{hyperref}
\usepackage[nameinlink]{cleveref}
\usepackage{caption}
\usepackage{enumerate}


\newtheorem{lemma}{\indent Лемма}
\newtheorem{theorem}{\indent Теорема}
\newtheorem{corollary}{\indent Следствие}
\newtheorem{problem}{\indent Задача}
\newtheorem{remark}{\indent Замечание}
\newtheorem{definition}{\indent Определение}
\newtheorem{proposition}{\indent Утверждение}
\newtheorem{example}{\indent Пример}
\newtheorem{notation}{\indent Обозначение}

\crefname{lemma}{л.}{л.}
\crefname{theorem}{теор.}{теор.}
\crefname{corollary}{след.}{след.}
\crefname{definition}{опр.}{опр.}
\crefname{proposition}{утв.}{утв.}
\crefname{example}{прим.}{прим.}
\crefname{notation}{обозн.}{обозн.}
\crefname{remark}{замеч.}{замеч.}

\crefname{theorem}{Теорема}{Теорема}
\newcommand{\order}[2]{#1_{(#2)}}

\newcommand{\T}{^{\text{\tiny\sffamily\upshape\mdseries T}}}

\graphicspath{{./img/}}


\def\XYtext(#1,#2)#3{\rlap{\kern#1\lower-#2\hbox{#3}}}

% Переопределение вставки графики
\newcounter{PictureNo}

\hyphenpenalty 100
\tolerance 10000


\newenvironment{Proof}%
    {\par\noindent{\bf Доказательство.}}%
    {\hfill$\scriptstyle\blacksquare$}

\usepackage{bbm}
\bibliographystyle{unsrt}


\DeclareCaptionLabelSeparator{dash}{ –- }

\addto\captionsenglish{\renewcommand{\figurename}{Рисунок}}
\addto\captionsenglish{\renewcommand{\tablename}{Таблица}}
\addto\captionsenglish{\renewcommand{\bibname}{Список использованных источников}}
\captionsetup[figure]{labelfont=small,textfont=small,labelsep = dash}
\captionsetup[table]{labelfont=small,textfont=small,labelsep = dash}

\addto\captionsenglish{\renewcommand{\proofname}{Доказательство}}

\makeatletter % эта строка НЕОБХОДИМА!
\renewcommand{\@chapapp}{Глава} 
\addto\captionsenglish{% Replace "english" with the language you use
  \renewcommand{\contentsname}%
    {Оглавление}%
}

\titleformat{\chapter}[display]
{\normalfont\bfseries\center}
{Глава \thechapter. }{0em}{}

\titleformat{\section}[block]
{\normalfont\bfseries\center}
{\thesection. }{0em}{}

\titleformat{\subsection}[block]
{\normalfont\bfseries\center}
{\thesubsection. }{0em}{}

\newcommand\thefontsize[1]{{#1 The current font size is: \f@size pt\par}}

\begin{document}
\begin{center}
\hfill \break
\small{Федеральное государственное автономное образовательное учреждение \linebreak 
высшего образования}\\ 
\small{<<Московский физико-технический институт (государственный университет)>>}\\
\hfill \break
\small{Факультет инноваций и высоких технологий}\\
\small{Кафедра анализа данных}\\
\end{center}
\small{\textbf{Направление подготовки:} 01.03.02 Прикладная математика и информатика}\\
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\begin{center}
\normalsize{\textbf{Автоматическая расстановка ударений в словах}}\\
\small{Бакалаврская работа}\\
\hfill \break
\hfill \break
\end{center}
 
\hfill \break
 
\begin{flushright}
\small{ 
\begin{tabular}{rl}
\textbf{Обучающийся:} & Захаров Александр Сергеевич \\
 & \underline{\hspace{3cm}} \\\\
\textbf{Научный руководитель:} & Конушин Антон Сергеевич\\
            & к.ф.-м.н., доцент \\
 & \underline{\hspace{3cm}} \\\\
\end{tabular}
}
\end{flushright}

\hfill \break
\hfill \break
\hfill \break
\hfill \break
\begin{center} Москва 2018 \end{center}
\thispagestyle{empty} % выключаем отображение номера для этой страницы
 
% КОНЕЦ ТИТУЛЬНОГО ЛИСТА
 
\newpage
\chapter*{Аннотация}


Ударение является важным элементом речи. В русском языке их расстановка по написанию слова является сложной задачей, а ее решение необходимо для задач синтеза и распознавания речи. В работе исследуется нейросетевой подход к данной задаче. Было опробовано несколько различных архитектур на основе рекуррентых нейронных сетей. Мы исследовали символьные и слоговые представления данных. Нами была исследовано влияние размера обучающей выборки на качество работы модели.


\tableofcontents{}


\chapter*{Введение}
\addcontentsline{toc}{chapter}{Введение}
Ударение в словах – важнейший элемент устной, письменной и внутренней речи. В русском языке оно играет исключительно важную роль, так как благодаря ему мы можем различать слова. Одной из сложностей русского языка является его свободное ударение, которое не закреплено за каким-либо определенным слогом или морфемой слова. Любой слог может выделяться фонетически. К тому же ударение  может меняться с изменением грамматической формы слова. Как отмечает лингвист Н. А. Еськова, «слова с подвижным ударением в русском языке исчисляются сотнями. В процентном отношении это немного, но среди них много чрезвычайно употребительных, поэтому в речи они достаточно заметны» \cite{eskina}. Например: фл\'{а}г — фл\'{а}га — фл\'{а}ги; но вр\'{а}г — враг\'{а} — враг\'{и} 

Есть языки, где ударение  всегда на одном и том же слоге — такое ударение называют фиксированным. Например, во французском ударение всегда на последнем слоге, в польском — на предпоследнем, в чешском — на первом. В русском языке аналогичные правила весьма размыты, поэтому если человек не знает, как правильно ставить ударение в слове, то по одному только его внешнему облику сделать правильный выбор бывает сложно.  Нет общих правил ударения и в заимствованных для русского языка словах . Иногда оно меняет свое место по сравнению с ударением в языке-источнике: ноутб\'{у}к, скелет\'{о}н, футб\'{о}л, хокк\'{е}й. А иногда сохраняет: буль\'{о}н, гардер\'{о}б, жалюз\'{и}.
Расстановка ударений, как часть задачи предсказания произношения, - важная составляющая  приложений, таких как: автоматическое распознавание речи, синтез речи, транслитерация. Кроме того – это необходимо всем, изучающим русский язык.


\newpage

\chapter{Литературный обзор}

Работы по предсказанию постановки ударений в словах велись в двух направлениях. На основе лингвистических правил \cite{church, williams} и на основе анализа данных, где модели строятся напрямую из текстов с обозначенными ударениями. 

В русском языке сохраняется множество индо-европейских шаблонов ударений. Чтобы узнать ударение морфологически сложного слова, состоящего из основы и окончания, необходимо узнать является ли основа ударной и на какой слог падает в ней ударение, либо ударным является окончание \cite{halle}.
\section{Предсказание ударения в слове на основе ранжирования}
Авторы \cite{hall} рассмотрели проблему расстановки ударений, как задачу ранжирования. В своем исследовании  они опирались на более раннюю статью \cite{dou}. Из каждого слова выделяются гласные буквы и они предполагаются, как возможные варианты постановки ударений. Целью модели является отранжировать варианты так, чтобы верная гипотеза имела наименьший ранг.

Для ранжирования гипотез применялось Maximum Entropy ранжирование \cite{collins}. Во время обучения модели ей подавался набор правильных гипотез и их признаков. Во время предсказания в модель подавались все гипотезы, и в качестве верной выбиралась гипотеза с максимальным предсказанным результатом. В качестве основы для ранжирования использовалась линейная модель, вместо SVM, так как она более эффективна с вычислительной точки зрения для обучения и применения. 

В базовой статье \cite{dou} признаками являлись триграммы для гласных букв следующего вида: предыдущая согласная, если она есть, гласная буква, следующая за ней слогласная, если она есть (Dou). На основе лигвистического исследования в данной статье авторы добавили следующие признаки: для каждого слова взяты все начальные и конечные части (уже - у, уж, уже, е, же) (Affix). Также эти признаки добавлены в следующем виде: все буквы заменены на их абстрактные фонетические классы (представлены в \cref{table:phon_class})({Abstr Aff}).
\begin{table}[H]
		\caption{ Абстрактные фонетические классы}
	
	\begin{small}
		\begin{center}
			\begin{tabular}{|c|c|}
				\hline
				  Класс   &           Буквы            \\ \hline
				  vowel   & а, е, и, о , у, э, ю, я, ы \\ \hline
				  stop    &      б, д, г, п, т, к      \\ \hline
				  nasal   &            м, н            \\ \hline
				fricative &    ф, с, ш, щ, х, з, ж     \\ \hline
				hard/soft &            ъ, ь            \\ \hline
				   yo     &             ё              \\ \hline
				semivowel &            й, в            \\ \hline
				 liquid   &            р, л            \\ \hline
				affricate &            ц, ч            \\ \hline
			\end{tabular}
		\end{center}
	\end{small}
	\label{table:phon_class}
\end{table}

В качетсве данных авторы использовали Грамматический словарь русского языка Зализняка\cite{zaliz}, разбитый на обучающую и тестовую выборки. Из тестовой выборки также были отдельно выделены те слова, которые не встречались в обучающей выборке,  и для них также были получены  результаты. Результаты экспериментов представлены в \cref{table:range_res}.
\begin{table}[H]
		\caption{Результаты ранжирования}

	\begin{small}
		\begin{center}
			\begin{tabular}{|c|c|}
				\hline
				       Признаки         &             Accuracy score             \\ \hline
				\multicolumn{2}{|c|}{Тестовая выборка}                           \\ \hline
				          Dou           &                 0.972                  \\ \hline
				          Aff           &                 0.987                  \\ \hline
				     Aff+Abstr Aff      &                 0.987                  \\ \hline
				     Dou et al+Aff      &                 0.987                  \\ \hline
				Dou et al+Aff+Abstr Aff &                 0.987                  \\ \hline
				\multicolumn{2}{|c|}{Слова не встречавшиеся в обучающей выборке} \\ \hline
				          Dou           &                 0.806                  \\ \hline
				          Aff           &                 0.798                  \\ \hline
				     Aff+Abstr Aff      &                 0.810                  \\ \hline
				     Dou et al+Aff      &                 0.823                  \\ \hline
				Dou et al+Aff+Abstr Aff &                  0.89                  \\ \hline
			\end{tabular}
		\end{center}
	\end{small}
	\label{table:range_res}
\end{table}

Таблица показывает  влияние взаимодействия признаков на обобщающую способность модели, и лучший результат достигнут при использовании всех признаков.

Недостатками этой работы является неиспользование контекста для определения места ударения. При подсчете результатов никак не учитывалась частота употребления слов в текстах языка, использовался просто его лексический набор.

\section{Предсказание ударения в слове при помощи конечного преобразователя}

Целью авторов\cite{reynolds} была разработка модели, которая могла быть помочь людям изучать русский язык. Авторы решили, что в некоторых словах ударение может быть пропущено. Считалось, что неправильное ударение может быть хуже, чем его отсутствие для человека, осваиваюющего новый язык.

Модель состояла из двух частей: конечного преобразователя \cite{koskenniemi, karttunen}, который из полученного слова генерировал все возможные, корректные по его мнению, позиции ударения. Далее при помощи формальной грамматики \cite{karlsson} удалялись варианты, которые не подходили по контексту. Если после применения этой процедуры оставался один вариант прочтения, то он и выбирался как финальный. Если ни одного -- то ударение в слове не проставлялось. Если же вариантов было несколько, -- то в зависимости от эксперимента выбиралась дальнейшая стратегия. 

Авторами использовался корпус текстов, состоящий из 7689 слов с размеченными ударениями, это были тексты для начинающих изучать русский язык. Также для обучения модели применялся Грамматический словарь русского языка Зализняка \cite{zaliz}. 

Описание экспериментов:
\begin{itemize}
	\item \textbf{bare:} при нескольких возможных вариантах прочтения  слова, ударение в слове не проставлялось.
	\item \textbf{safe:}  при нескольких возможных вариантах прочтения, ударение в слове выставлялось, если во всех них ударение падало на один и тот же слог.
	\item \textbf{randReading:} при нескольких возможных прочтениях, случайно выбиралось одно с вероятностью выбора варианта равной частоте встречаемости этого варианта в тексте.
	
	\item \textbf{freqReading:} при нескольких возможных прочтениях, выбирается вариант с максимальной  частотой встречаемости среди всех  вариантов в тексте.
\end{itemize}


Эксперименты были проведены при использовании формальной грамматики с учетом контекста и без него. Результаты представлены в \cref{table:final_state_result}. Для слов, которые не встретились в словаре, применялось простое правило постановки ударения: ударение падает на последню гласнуюю, после которой идет согласная. Это является наиболее вероятным вариантом ударения в русском языке \cite{lavitskaya}. В результатх это отображено как guessSyl.

\begin{table}[H]
	\caption{Результаты применения конечного преобразователя}
	
	\begin{small}
		\begin{center}
			\begin{tabular}{|c|c|c|c|}
				\hline
				     Эксперимент      & Accuracy score & Доля ошибок & Доля пропущенных слов \\ \hline
				\multicolumn{4}{|c|}{Без грамматики}                                         \\ \hline
				        bare          &     30.43      &    0.17     &         69.39         \\ \hline
				        safe          &     90.07      &    0.49     &         9.44          \\ \hline
				     randReading      &     94.34      &    3.36     &         2.30          \\ \hline
				     freqReading      &     95.53      &    2.59     &         1.88          \\ \hline
				randReading+guessSyll &     94.99      &    4.05     &         0.96          \\ \hline
				freqReading+guessSyll &     95.83      &    3.46     &         0.72          \\ \hline
				\multicolumn{4}{|c|}{С грамматикой}                                          \\ \hline
				        bare          &     45.78      &    0.44     &         53.78         \\ \hline
				        safe          &     93.21      &    0.74     &         6.058         \\ \hline
				     randReading      &     95.50      &    2.59     &         1.90          \\ \hline
				     freqReading      &     95.73      &    2.40     &         1.88          \\ \hline
				randReading+guessSyll &     95.92      &    3.33     &         0.74          \\ \hline
				freqReading+guessSyll &     96.15      &    3.14     &         0.72          \\ \hline
			\end{tabular}
		\end{center}
	\end{small}
	\label{table:final_state_result}
	
\end{table}

При использовании модели без формальной грамматики полнота гипотез составила 97.55\%, что является максимумом результата для данной модели. При использовании грамматики полнота составила 97.35\%. Эти результаты являются потолком для соответствующих экспериментов. Совмещение всех моделей и предсказываение методом FreqReading  позволило получить наибольший процент правильных ударений. Метод расстановки ударений для неизвестных слов в данном случае имеет точность всего 21\%. При этом высокая точность была достигнута за счет расстановки ударений почти во всех словах, что соответственно повысило уровень ошибок. 

Метод, представленный в этой статье, является попыткой улучшить словарный метод, путем разрешения неоднозначностей в омографах при помощи формальной грамматики. При этом для слов, которые не  встретились в словаре, работает очень простой и слабый алгоритм. В этом случае качество получается очень низким. Недостатком является также использование небольшого закрытого корпуса текстов. А так как использовались тексты для начинающих изучать язык, их словарь скорее всего был достаточно мал. Не ясна цель проведения эксперимента RandReading, так как несложно показать строго математически, что метод FreqReading всегда дает большую вероятность правильного ответа.

\section{Предсказание ударения в слове при помощи символьной нейронной сети}
\label{global_desc}
	В качестве основы для модели авторами \cite{ponomareva} использовалась  символьная двусторонняя рекуррентная нейронная сеть на основе LSTM-модулей. На вход подавалась матрица размера [длина фразы; число возможных символов]. Для кодирования символов было применено one-hot кодирование. Авторами выбрана следующая архитектура: к входной матрице применяется двусторонняя рекуррентная нейронная сеть, сконкатенированные вектора, полученные от рекуррентного слоя подаются в полносвязные слой с softmax активацией. На выходе получается вектор размера равного длине фразы, соответствующий распределению вероятностей постановки ударения в конкретной позиции (\cref{fig:base_global}).

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.5\linewidth]{Baseline}
	\end{center}
	\caption{\small{Архитектура сети}}
	\label{fig:base_global}
\end{figure}

Для разных экспериментов использовался грамматический словарь русского языка Зализняка \cite{zaliz} и база данных акцентологической разметки в составе национального корпуса русского языка \cite{grishina}. 

Авторами были проведены следующие эксперименты.
\begin{enumerate}[  1{)} ]
	\item \textbf{Обучение и предсказание на основе словаря.} Словарь Зализняка был разделен на обучающую и тестовую выборки в соотношении 2:1.
	\item \textbf{Обучение и предсказание на основе акцентологического корпуса.} С корпусом было проведено два эксперимента, в первом в качестве фразы использовалось только само слово. Во втором же, к нему были дописаны три последние буквы из слова, которое идет перед ним в предложении, если такое было. Основная разница между моделями с контекстом и без него может быть видна только на омографах. Как видно из результатов, модель успешно использует контекст для расстовки ударения в омографах во многих случаях.
\end{enumerate}
Результаты этих экспериментов представлены в \cref{table:base_text}.

\begin{table}[H]
		\caption{Результаты нейросетевой модели}
	
	\begin{small}
		\begin{center}
			\begin{tabular}{|c|c|c|c|}
				\hline
				   Эксперимент     & Модель с контекстом & Модель без контекста &  \\ \hline
				Словарь Зализняка  &          -          &        0.751         &  \\ \hline
				      Текст        &        0.979        &        0.977         &  \\ \hline
				Омографы в текстах &        0.819        &         0.79         &  \\ \hline
			\end{tabular}
		\end{center}
	\end{small}
	\label{table:base_text}
\end{table}

Как видно из представленных данных, нейросетевая модель успешно справляется с использованием контекста для расстановки ударений в омографах. Недостатками же представленной модели является очень простая архитектура и отсутствие работы с текстом. Далее мы будем использовать эту модель как базовую для сравнения результатов. 


\newpage
\chapter{Экспериментальная часть}
\section{Используемые данные}
\label{prepare}

Во всех экспериментах в качестве источника данных мы использовали базу данных акцентологической разметки в составе национального корпуса русского языка \cite{grishina}. С каждым предложением в тексте были произведены следующие преобразования:
\begin{enumerate}[  1{)} ]
	\item Все буквы приведены к строчным;
	\item Предложение разбито на смысловые подпредложения с использованием  в качестве разделителей знаков препинания. После этого все знакия препинания удаляются;
	\item Подпредложения, содержавшие  иные символы кроме букв  кириллицы удалены;
	\item По правилам эксперимента из получившихся подпреложений собирались фразы.
\end{enumerate}

Итоговые данные состояли из 3285455  слов. Все данные были разделены на 3 части: обучающую выборку (2299818 слов), валидационную выборку, применяемую для подбора параметров во время обучения модели (49281 слов) и тестовую  выборку, на которой измерялся конечный результат (936356 слов).

Омографы среди всех слов в нашей выборке составляют 3.31\%. Распределение долей слов по длинам представлено в \cref{table:length_gen}. Аналогичное распределение для омографов представлено в \cref{table:length_homo}.

\begin{table}[H]
	\caption{Распределение слов по числу слогов}
	
	\begin{small}
		\begin{center}
			\begin{tabular}{|c|c|}
				\hline
				Число слогов & Доля слов \\ \hline
				     2       &   0.474   \\ \hline
				     3       &   0.308   \\ \hline
				     4       &   0.144   \\ \hline
				     5       &   0.053   \\ \hline
				     6       &   0.015   \\ \hline
				     7       &   0.003   \\ \hline
				     8       &   0.001   \\ \hline
				     9       & $10^{-4}$ \\ \hline
			\end{tabular}
			\end{center}
		\end{small}
	\label{table:length_gen}
\end{table}	
\begin{table}[H]
	\caption{Распределение омографов по числу слогов}
	
	\begin{small}
		\begin{center}
			\begin{tabular}{|c|c|}
				\hline
				Число слогов & Доля слов \\ \hline
				     2       &   0.736   \\ \hline
				     3       &   0.212   \\ \hline
				     4       &   0.051   \\ \hline
			\end{tabular}
		\end{center}
	\end{small}
	\label{table:length_homo}
\end{table}	



\section{Метрики}
Основной метрикой, используемой для оценки окончательного качества, является \textit{Accuracy score}  (\ref{eq:acc}). 

Обозначим позицию ударения во фразе как $y_i$. Позицию ударения, предсказанную моделью, обозначим как $y^*_i$. Число фраз в выборке обозначим как $N$

\begin{align}
\label{eq:acc} ACC &= \frac{\sum\limits_{i=1}^{N}I \left\{y_i = y^*_i\right\}}{N} 
\end{align}

\section{Локальная модель}
В качестве самой простой нейросетевой архетектуры нами была выбрана данная архитектура. Ее можно считать упрощением базовой модели \cite{ponomareva}.
\subsection{Архитектура модели}
К входным данным применяется двустронняя рекуррентная нейронная сеть на основе LSTM-модулей. Далее к каждому промежуточному вектору применяется один и тот же полносвязный слой с softmax активацией с размерностью выхода 2. Первую компоненту этого вектора мы интерпретируем, как вероятность того, что в данной позиции нет ударения, вторую -- как то что оно есть (\cref{fig:local}). Эту архитектуру далее мы будем называть локальной моделью. 

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.5\linewidth]{Local}
	\end{center}
	\caption{\small{Архитектура локальной модели}}
	\label{fig:local}
\end{figure}


\subsection{Посимвольный эксперимент}
\textbf{Входные данные:} На вход модели подается слово, в котором мы хотим поставить ударение. Четыре последние буквы предыдущего слова, если оно есть и четыре последние буквы следующего слова, если оно есть. Четыре буквы мы используем, потому что это  длина  окончания в русском языке. Окончание может нам помочь определить форму слова, что необходимо для удаления неоднозначности при расстановке ударения в большинстве омографов. Пример входных данных и ответа представлен в \cref{table:local_char_ex}

\begin{table}[H]
	\caption{Пример данных для символьной модели}
	\begin{small}
		\begin{center}
			\begin{tabular}{|c|l|}
				\hline
				Фраза &{\usefont{T2A}{PTMono-TLF}{m}{n}ятой руки было} 
										 \\ \hline
				Матрица ответа      &     {\usefont{T2A}{PTMono-TLF}{m}{n}11111111011111 }     \\ 
				& {\usefont{T2A}{PTMono-TLF}{m}{n}00000000100000 } \\ \hline
			\end{tabular}
		\end{center}
	\end{small}
	
	\label{table:local_char_ex}
\end{table}

Позиция ударения во фразе выбиралась, как позиция с максимальной вероятностью второго класса.

Результаты этого эксперимента и сравнение с базовой моделью представлены в \cref{table:local_char}

\begin{table}[H]
		\caption{Результаты локальной и глобальной символьной модели}
	
	\begin{small}
		\begin{center}
			\begin{tabular}{|c|c|c|}
				\hline
				Число слогов & Локальная модель & Глобальная модель \\ \hline
				\multicolumn{3}{|c|}{Все слова}                     \\ \hline
				     2       &      0.961       &       0.983       \\ \hline
				     3       &      0.940       &       0.977       \\ \hline
				     4       &      0.947       &       0.976       \\ \hline
				     5       &      0.960       &       0.977       \\ \hline
				     6       &      0.958       &       0.973       \\ \hline
				     7       &      0.924       &       0.955       \\ \hline
				     8       &      0.866       &       0.923       \\ \hline
				     9       &      0.809       &       0.952       \\ \hline
				  среднее    &      0.952       &       0.979       \\ \hline
				\multicolumn{3}{|c|}{Омографы}                      \\ \hline
				     2       &      0.839       &       0.810       \\ \hline
				     3       &      0.774       &       0.844       \\ \hline
				     4       &      0.787       &       0.847       \\ \hline
				  среднее    &      0.821       &       0.819       \\ \hline
			\end{tabular}
		\end{center}
	\end{small}
	\label{table:local_char}
\end{table}	

Наше предположение о том, что эта модель более слабая, чем глобальная подтвердилось. При этом, благодаря изменению, заключающемуся в изменениии использования контекста, результаты на омографах удалось улучшить.

\subsection{Эксперимент с предложением}

Для исследования влияния длины контекста на качество расстановки ударений был проведен следующий эксперимент. Контекстом являются не окончания соседних слов, а все подпредложение (часть предложения между знаками препинаниия). Является ли введение в контекст других слов, кроме соседних, значимым, будет ясно по результатам этого эксперимента.

\textbf{Входные данные:} На вход модели подается подпредложение, описание построения которого находится в \cref{prepare}. При этом модель должна расставить ударения во всех словах в подпредложении.

Для получения итогового результата из вектора с вероятностями мы в каждом слове выбирали символ с наибольшей вероятностью того, что на него падает ударение. Это, в отличие от отсечения по границе, позволяет добиться того, что в каждом слове находится ровно одно ударение. Пример вхордных данных и матрицы ответа представлен в \cref{table:local_sent_ex}

Результаты этого эксперимента и их сравнение с локальной символьной моделью представлены в \cref{table:local_sent}.

\begin{table}[H]
	\caption{Пример данных для модели по предложениям}
	\begin{small}
		\begin{center}
			\begin{tabular}{|c|l|}
				\hline
				    Фраза      & {\usefont{T2A}{PTMono-TLF}{m}{n}позволяет добиться того}  \\ \hline
				Матрица ответа & {\usefont{T2A}{PTMono-TLF}{m}{n}11111101111110111111110 } \\
				               & {\usefont{T2A}{PTMono-TLF}{m}{n}00000010000001000000001 } \\ \hline
			\end{tabular}
		\end{center}
	\end{small}
	
	\label{table:local_sent_ex}
\end{table}


\begin{table}[H]
		\caption{Результаты локальной символьной модели и локальной модели по предложениям}
	\begin{small}
		\begin{center}
			\begin{tabular}{|c|c|c|}
				\hline
				Число слогов & Символьная модель & Модель по предложениям \\ \hline
				\multicolumn{3}{|c|}{Все слова}                          \\ \hline
				     2       &      0.961       &         0.897          \\ \hline
				     3       &      0.940       &         0.891          \\ \hline
				     4       &      0.947       &         0.902          \\ \hline
				     5       &      0.960       &         0.927          \\ \hline
				     6       &      0.958       &         0.925          \\ \hline
				     7       &      0.924       &         0.898          \\ \hline
				     8       &      0.866       &         0.855          \\ \hline
				     9       &      0.809       &         0.647          \\ \hline
				  среднее    &      0.952       &         0.898          \\ \hline
				\multicolumn{3}{|c|}{Омографы}                           \\ \hline
				     2       &      0.839       &         0.831          \\ \hline
				     3       &      0.774       &         0.754          \\ \hline
				     4       &      0.787       &         0.775          \\ \hline
				  среднее    &      0.821       &         0.812          \\ \hline
			\end{tabular}
		\end{center}
	\end{small}

	\label{table:local_sent}
\end{table}

Увеличившийся контекст не привел к повышению результата, при этом из-за усложниившейся задачи качество упало. При этом эта модель требует на порядок больше времени для обучения. А для достижения схожего качества необходимо значительное увеличениие числа LSTM модулей. Поэтому далее мы откажемся от рассмотрения данного подхода к подготовке данных.
\subsection{Слоговая модель}
\label{syl_descr}
В русском языке ударение может падать только на гласные буквы. Модели приходилось также учитывать то, что на согласные буквы ударение падать не может. Из-за этого увеличивалась сложность модели и количество информации, которое она должна хранить. Также из-за этого увеличивалось время обучения.

Одним из вариантов решения этой проблемы является замена символьной модели на слоговую, то есть на вход модели будут подаваться закодированные слоги, а не символы.

Деление на слоги слов в русском языке однозначно установлено \cite{litnevskaya}, поэтому преобразование данных будет детерменированно. 

После преобразования получилось 14083 слога.

\textbf{Входные данные:} Формат входных данных аналогичен формату, примененному в локальной символьной модели. Пример входных данных и матрицы ответа представлен в \cref{table:local_syl_ex}.

Результаты этого эксперимента, а также их сравнение с результатами локальной и глобальной сисвольной моделей представлены в \cref{table:local_syl}.

\begin{table}[H]
	\caption{Пример данных для слоговой модели}
	\begin{small}
		\begin{center}
			\begin{tabular}{|c|l|}
				\hline
				Фраза &{\usefont{T2A}{PTMono-TLF}{m}{n}ля\_ет до\_би\_ться то\_го} 
				\\ \hline
				Матрица ответа      &     {\usefont{T2A}{PTMono-TLF}{m}{n} \ 1\ 1 1 1\ \  0\ \ \ \ 11 1\ \ 1 }     \\ 
				&  {\usefont{T2A}{PTMono-TLF}{m}{n}\ 0\ 0 0 0\ \  1\ \ \ \ 00 0\ \ 0} \\ \hline
			\end{tabular}
		\end{center}
	\end{small}
	
	\label{table:local_syl_ex}
\end{table}

\begin{table}[H]
		\caption{Результаты локальной и глобальной символьной модлелей с локальной слоговой моделью}
	
	\begin{small}
		\begin{center}
			\begin{tabular}{|c|c|c|c|}
				\hline
				Число слогов & Слоговая модель & Локальная модель & Глобальная модель \\ \hline
				\multicolumn{4}{|c|}{Все слова}                                       \\ \hline
				     2       &      0.985      &      0.961       & 0.983             \\ \hline
				     3       &      0.972      &      0.940       & 0.977             \\ \hline
				     4       &      0.972      &      0.947       & 0.976             \\ \hline
				     5       &      0.976      &      0.960       & 0.977             \\ \hline
				     6       &      0.977      &      0.958       & 0.973             \\ \hline
				     7       &      0.947      &      0.924       & 0.955             \\ \hline
				     8       &      0.899      &      0.866       & 0.923             \\ \hline
				     9       &      0.843      &      0.809       & 0.952             \\ \hline
				  среднее    &      0.978      &      0.952       & 0.979             \\ \hline
				  \multicolumn{4}{|c|}{Омографы}                                        \\ \hline
				  
				     2       &      0.889      &      0.839       & 0.810             \\ \hline
				     3       &      0.832      &      0.774       & 0.844             \\ \hline
				     4       &      0.843      &      0.787       & 0.847             \\ \hline
				  среднее    &      0.877      &      0.821       & 0.819             \\ \hline
			\end{tabular}
		\end{center}
	\end{small}
	\label{table:local_syl}
\end{table}

Применение слогового кодирования позволило без изменения архитектуры модели повысить качество расстановки ударений на 2.6\% для всех слов. Это позволило этой модели сравняться по качеству с глобальной символьной моделью. При этом качество расстоновки удаления в омографах у этой модели выше. Из этого всего можно сделать вывод, что обучение модели на слогах вместо символов может помочь повысить результат  без изменения архитектуры. Это можно объяснить тем, что модель учит векторные представления для слогов отдельно в embedding слое, а не пытается выучить подобные взаимосвязи в рекуррентном слое. Или  в слоге содержится больше информации, чем в одной букве.

\section{Глобальная модель}
\subsection{Архитектура модели}
Архитектура этой модели совпадает с архитектурой базовой модели. Подробно она описана в \cref{global_desc}. 
Схема архитектуры представлена на \cref{fig:base_global2}

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.5\linewidth]{Baseline}
	\end{center}
	\caption{\small{Архитектура глобальной модели}}
	\label{fig:base_global2}
\end{figure}

\subsection{Символьная модель}
Эта модель является нашей базовой. Опиисание данных и результаты этой модели представлены выше в литературном обзоре в \cref{global_desc}

\subsection{Слоговая модель}
Использование слогов, как базовой входной единицы модели показало положительный результат при использовании с локальной моделью. Мы рассчитываем также получть положительный результат с глобальной моделью.

Результаты этого эксперимента, и их сравнение с  глобальной символьной моделью и локальной слоговой моделью представлены \cref{table:global_syl}.

\begin{table}[H]
	\caption{Результаты локальной и глобальной слоговой модлелей с глобальной символьной моделью}
	
	\begin{small}
		\begin{center}
			\begin{tabular}{|c | c | c|c|}
				\hline
				Число слогов & Глобальная модель & Локальная модель & Символьная модель \\ \hline
				\multicolumn{4}{|c|}{Все слова}                                       \\ \hline
				2       & 0.985  &      0.985      & 0.983             \\ \hline
				3       &   0.978 &      0.972      & 0.977             \\ \hline
				4       &   0.977 &      0.972      & 0.976             \\ \hline
				5       &  0.977 &      0.976      & 0.977             \\ \hline
				6       &  0.970 &      0.977      & 0.973             \\ \hline
				7       &  0.945 &      0.947      & 0.955             \\ \hline
				8       &  0.895 &      0.899      & 0.923             \\ \hline
				9       & 0.849 &      0.843      & 0.952             \\ \hline
				среднее    & 0.981 &      0.978      & 0.979             \\ \hline
				\multicolumn{4}{|c|}{Омографы}                                        \\ \hline
				
				2       & 0.893&      0.889      & 0.810             \\ \hline
				3       & 0.847&      0.832      & 0.844             \\ \hline
				4       & 0.852&      0.843      & 0.847             \\ \hline
				среднее    & 0.882 &      0.877      & 0.819             \\ \hline
			\end{tabular}
		\end{center}
	\end{small}
	\label{table:global_syl}
\end{table}
В глобальнной модели использование слогового кодирования также дало повышение результата по сравнению с символьной моделью. Поэтому далее символьные модели мы больше не будем рассматривать.  Также это первая модель, которая показала лучший результат, чем базовая модель. 

Глобальная модель  по сравнениию с локальной имеет большие проблемы с расстановкой ударения в словах, содержащих более чем из 7 слогов. Это можно объяснить тем, что такие слова очень редко встречаются, и полносвязный слой выучивает, что последние слоги почти всегда безударные. Глобальной модели нужен гораздо более сильный сигнал, для постановки ударения в последний слог по сравнению с локальной. В локальной же модели полносвязный слой применяется в каждой позиции независимо, поэтому зависимость от позиции менее выражена. В символьной модели этот эффект меньше проявлен, так как длина слова в слогах и буквах не связана линейно, поэтому более длинные слова там встречаются чаще. 

\section{Модель с attention слоем}
Значительного увеличения качества в машинном переводе удалось добится благодаря замене encoder-decoder архитектуры на основе только рекуррентного слоя добавление attention механизма. 
\subsection{Описание attention механизма}
Описание attention мехаизма производится на основе одной из первых статей, где он был представлен \cite{bahdanau}. Описанный далее attention механизм также известен как soft attention.

Целью attention-механизма является предсказание вектора текущего сотояния($a_i$), который будет проинтерпретирован в соответствие с поставленной задачей. При этом на вход ему подается входной вектор состояния ($v_i$), вектора промежуточных состояний кодирующего слоя ($h_t, t \in [0; l)$, где $l$ - длина входных данных), чаще всего это слой на основе рекуррентных модулей. 
Сначала для каждого промежутного вектора вычисляется его значимость по отношение к текущему состоянию (\ref{eq:score}). Далее полученные значимости преобразуются в веса для каждой позиции  $\alpha_{i,t}$ при помощи sotmax преобразования (\ref{eq:prob}). Взвешенная сумма промежуточных векторов $h_t$ с весами $\alpha_{i,t}$ называется контекстным вектором $c_i$ \ref{eq:att_vec}. Выходной вектор получается из контекстного линейным преобразованием и затем покоординатным нелинейным преобразованием. В данном случае -- это гиперболический тангенс (\ref{eq:out_vector}).

\begin{gather}
\label{eq:score} score(h_t, v_i) = v_i^Ttanh\left( W_1 v_i + W_2 h_t\right) \\
\label{eq:prob} \alpha_{i,t}=\frac{\exp\left(score\left(h_t,   v_i\right) \right) }{\sum\limits_{k=0} ^{l-1} \exp\left(score\left(h_k,   v_i\right) \right)}\\
\label{eq:att_vec} c_i = \sum\limits_{t=0}^{l-1} \alpha_t h_t \\
\label{eq:out_vector} a_i = tanh\left(W_3 c_i \right)  
\end{gather}
 
В представленных выше уравнениях $W_i$ - матрицы линейных преобразований.

Благодаря подсчету весов в каждом промежуточном состоянии можно визуализировать то, какие части входа были наиболее важными для получения результата в конкретной позиции. При использовании attention механизма в нейросетевом машинном переводе такая визуализация получается хорошо интерпретируемой: для перевода текущего слова наиболее важными являются оригинальные слово или слова, которые передают его смысл. Пример при переводе с французского языка на английский язык представлен на \cref{fig:att_map}

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.5\linewidth]{AttentionMap}
	\end{center}
	\caption{\small{Визуализация весов $\alpha_{i,t}$ при переводе}}
	\label{fig:att_map}
\end{figure}


\subsection{Архитектура модели}
Так как нам нужно предсказать только вектор распределения вероятности ударения в конкретной позиции, мы не будем использовать декодирующую сеть, а сразу однократным применением attention слоя получим искомый вектор.

Модель состоит из двустороннего рекуррентного слоя на основе LSTM модулей. К выходному вектору рекуррентного слоя применяется полносвязный слой, полученный вектор будет рассматривать как вектор текущего состояния для attention слоя. Далее следует attention слой и к полученному на его выходе вектору применяется полносвязный слой с softmax активацией.

Схема этой архитектуры представлена на \cref{fig:att}

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.5\linewidth]{Attention}
	\end{center}
	\caption{\small{Архитектура сети с attention}}
	\label{fig:att}
\end{figure}


Для нашей модели примение attention механизма можно считать отчасти объединениием локального и глобального подхода: мы используем все промежуточные состояния рекуррентного слоя, как в локальной модели. При этом предсказываем сразу распределение вероятностей постановки ударения в конкретную позицию, как в глобальной модели.
\subsection{Слоговая модель}
Слоговое кодирование показало себя лучше в других моделях, что было показано выше. Поэтому модель с attention обучалась только в таком режиме. Сравнение результатов с глобальной слоговой моделью представлено в \cref{table:global_att}

\begin{table}[H]
	\caption{Результаты слоговой глобальной модели и модели с attention}
	
	\begin{small}
		\begin{center}
			\begin{tabular}{|c | c| c |}
				\hline
								\multicolumn{3}{|c|}{Все слова}                                       \\ \hline
				
				Число слогов & Глобальная модель & Модель с attention \\ \hline
				     2       & 0.985             & 0.989              \\ \hline
				     3       & 0.978             & 0.982              \\ \hline
				     4       & 0.977             & 0.979              \\ \hline
				     5       & 0.977             & 0.980              \\ \hline
				     6       & 0.970             & 0.969              \\ \hline
				     7       & 0.945             & 0.936              \\ \hline
				     8       & 0.895             & 0.867              \\ \hline
				     9       & 0.849             & 0.747              \\ \hline
				  среднее    & 0.981             & 0.985              \\ \hline
				  				\multicolumn{3}{|c|}{Омографы}                                       \\ \hline
				  
				     2       & 0.893             & 0.900              \\ \hline
				     3       & 0.847             & 0.869              \\ \hline
				     4       & 0.852             & 0.846              \\ \hline
				  среднее    & 0.882             & 0.889              \\ \hline
			\end{tabular}
		\end{center}
	\end{small}
	\label{table:global_att}
\end{table}



Архитектура с attention дает улучшениие по сравнению с глобальной моделью. Однако проблемы глобальной модели со словами из большого числа слогов проявляются еще более сильно. Однако это является лучшим результатом, полученным нами.

\section{Анализ слогов}

Также интересным объектом для исследования являются векторные представления слогов, которые выучивает модель. Далее показана проекция в двумерное пространство, выполненая методом t-SNE, векторных представлений слогов, полученных от модели с attention(\cref{fig:tsne}).

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.5\linewidth]{Emb}
	\end{center}
	\caption{\small{Проекция векторных представлений слогов}}
	\label{fig:tsne}
\end{figure}

Кластерной структуры в слогах выделить неудается. Но можно выделить несколько групп по краям. Есть немало слогов, находящихся близко, имеющих большие общие подстроки. Например: вна, вну вны, вно. Из этого можно сделать вывод, что число слогов избыточно и уменьшив размер алфавита нам удасться выделить более мелкие осмысленные элемнты. Как это сделать будет рассмотрено ниже.

\section{Эксперименты с BPE токенизацией}
Одной из идей получения подслов, используемой в машинном переводе, является BPE токенизация. 
\subsection{Описание BPE токенизации}
С текстовым корпусом с изначальным алфавитом из букв проводятся следуюющие преобразовния \cite{sennrich}:
\begin{enumerate}[  1{)} ]
	\item К концам всех слов дописывается специальный символ <we>;
	\item Для всех пар соседниих символов подсчитывается число появлений в корпусе;
	\item Самая часто встречаемая пара добавляется в алфавит как новый символ и все ее появления в корпусе заменяется на него;
	\item Если размер алфавита меньше желаемого --  вернуться к пункту 2.
\end{enumerate}

\subsection{Применение BPE токенизации к слогам}
В нашей задаче в качестве отдельных слов мы будем рассматривать слоги. Размер алфавита был выбран 1000, так как при таком размере многие части слогов еще представляют собою отдельные символы, а  не слоги. Объединение в слоги произошло бы при слишком большом размере алфавита, приближающемся к числу слогов.

На таких данных была обучена модель с attention. К сожалению, не удалось добиться повышения результатов по сравнению с моделью со слогами. Получили ухудшение результата. Это представлено в \cref{table:bpe}. Поэтому от этой идеи пришлось отказаться.


\begin{table}[H]
	\caption{Результаты модели с attention при работе со слогами и BPE токенами}
	
	\begin{small}
		\begin{center}
			\begin{tabular}{|c | c | c|}
				\hline
				Тип слов  & Слоговая модель & Модель с BPE \\ \hline
				Все слова & 0.985           & 0.981        \\ \hline
				Омографы  & 0.900           & 0.853        \\ \hline
			\end{tabular}
		\end{center}
	\end{small}
	\label{table:bpe}
\end{table}




\section{Анализ ошибок}
Анализ ошибок будет проводится на основе нашей лучшей модели: слоговая модель с attention.
\subsection{Анализ влияния контекста}
Контекст является необходимым для определение ударения в омографах. 

Для анализа его влияния возьмем фразы из тестовой выборки, которые имеют и  левый, и правый контексты. Попытаемся проставить в них ударение в следующих случаях: с двумя контекстами, только с левым, только с правым и без контекста. Результаты этого эксперимента преставлены в \cref{table:cont}.

\begin{table}[H]
	\caption{Результаты расстановки ударения с разными типами контекста}
	
	\begin{small}
		\begin{center}
			\begin{tabular}{|c | c |}
				\hline
				Тип контекста  & Accuracy score \\ \hline
				Левый и правый & 0.986          \\ \hline
				    Левый      & 0.984          \\ \hline
				    Правый     & 0.977          \\ \hline
				Без контекста  & 0.976          \\ \hline
			\end{tabular}
		\end{center}
	\end{small}
	\label{table:cont}
\end{table}

Наличие левого контекста очень сильно влияет на результат предсказания. При этом правый контекст также повышает качество, но гораздо слабее.

Как говорилось выше, для модели с attention можно построить распределение весов для каждого элемента входных данных. Входную фразу можно разделить на следующие части: левый контекст, разделитель, слово, разделитель, правый контекст. Эти веса можно интерпретировать как важность этой части фразы для получения итогового результата. Такое распределение представлено для фраз, содержащих оба контекста в \cref{table:att_wei}

\begin{table}[H]
	\caption{Распределение весов attention слоя по частям фразы}
	
	\begin{small}
		\begin{center}
			\begin{tabular}{|c | c | c | c| c | c|}
				\hline
				Тип слов  & Левый контекст & Левый пробел & Слово & Правый пробел & Правый контекст \\ \hline
				Все слова & 0.008          & 0.294        & 0.551 & 0.140         & 0.005           \\ \hline
				Омографы  & 0.014          & 0.455        & 0.391 & 0.130         & 0.007           \\ \hline
			\end{tabular}
		\end{center}
	\end{small}
	\label{table:att_wei}
\end{table}

Видно, что вес разделителей очень высок, так как они являются ограничением на область выставления ударения, а также они разделяют семантически разные части фразы. В омографах вес левого контекста гораздо больше, чем в среднем по выборке. Про правый контекст такого вывода сделать нельзя, так как различия в значениях очень малы. Также вес самого слова в омографах ниже. 

Более высокий  вес слова  в обычных словах, по сравнению с омографами, можно объяснить тем, что в обычном слове его самого достаточно для простановки ударения, а в омографах контекст необходим для однозначной постаноки ударения.

Из этого мы можем сделать вывод о том, что левый контекст наиболее важен при растановке ударениий, роль правого контекста гораздо ниже.

\subsection{Работа модели с омографами}
Омографы являются наиболее сложными словами для расстановки ударений, так как ударение в них зависит не только от их написания, но и от контекста.  Для нашей модели с учетом того, что таких слов всего 5\% в текстах на них совершается 15\% ошибок.  У всех омографов всего 2 варианта ударения. При этом для слов, у которых доля одного вырианта выше 75-80\%-ов, всегда предсказывалось именно оно, и модели не удавалось определить омографную природу этого слова.  

В русском языке омографы можно раделить на несколько групп: слова разных частей речи (уж\'{е} - \'{у}же), словоформы одного слова (рук\'{и} - р\'{у}ки), и слова одной и той же части речи (з\'{а}мок - зам\'{о}к). Наиболее хороший результат наша модель показала при работе с омографами из разных частей речи (94.3\%). Для словоформ результат хуже и составил (84.7\%). Третья же группа омографов не может быть корректно обработана нашей моделью, так как левые и правые контексты таких слов совпадают.
\subsection{Работа модели со словами, которых не было в обучающей выборке}
Уникальных слов, которых не было в обучающей выборке встретилось 9805. Accuracy score: 0.838. Ошибки можно разделить на следующие группы: 
\begin{enumerate}[  1{)} ]
	\item заимствованнные слова, в которых правила ударения отличатся от правил русского языка;
	\item русские фамилии, в которых ударение совпадает с формой родительного падежа, но при этом они сами стоят в именительном;
	\item многоосновные слова, где модель не может выбрать, на какой корень должно ставиться ударение.
\end{enumerate}

\section{Active learning}
Отдельной темой для исследования является то, какого числа данных достаточно для достижения сходного качества. Техникой отбора данных, которая работает даже для неразмеченных данных, является Active learning \cite{shen}.

Идея заключается в том, что модели подается изначальный набор обучающих данных. На нем модель обучается. Из оставшихся данных выбираются примеры, в которых модель наиболее неуверена \ref{eq:n_prob}. Эти данные добавляются к обучающей выборке. И так продолжается до тех пор, пока качество не перестанет меняться от добавления новых данных. Данные выбираются по схеме выбора без возвращения. 

\begin{align}
\label{eq:n_prob} P &= \prod\limits_{i=1}^l \max(p_i, 1-p_i)
\end{align}

В нашем случае отдельно имеются омографы, на которых модели сложнее работать. Поэтому мы провели несколько разных экспериментов с active learning: в стартовых данных 100\% омографов, 50\%, 5\% и вообще без омографов. Для повышения скорости обучения эксперименты проводились с  глобальной слоговой моделью. 

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.8\linewidth]{All_scores}
	\end{center}
	\caption{\small{Результаты Active learning на всей тестовой выборке}}
	\label{fig:al_all}
\end{figure}
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.8\linewidth]{Homo_scores}
	\end{center}
	\caption{\small{Результаты Active learning на омографах}}
	\label{fig:al_homol}
\end{figure}
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.8\linewidth]{Homo_perc}
	\end{center}
	\caption{\small{Доля взятых омографов на каждом шаге}}
	\label{fig:al_perc}
\end{figure}

Как видно на \cref{fig:al_all} во всех случаях удалось добиться схожего качества с моделью, обученной на всех данных, кроме экстремального эксперимента со стартовыми данными, состоящимии только из омографов. При этом: чем больше процент омографов в стартовой выборке, тем выше получается качество расстановки ударений в них (\cref{fig:al_homol}). В середине обучения во всех экспериментах модели начинали собирать больше омографов в обучающую выборку, чем их среднее число в тексках. Это можно объяснить тем, что модель выучивает основные правила и начинает сомневаться в омографах (\cref{fig:al_perc}]).
 
 Такого качества удалось добиться на 20\% исходных данных. Важным вопросом при этом является сравнение active learning со случайный выбором примеров.  Такое сравнение представлено в \cref{table:al}

\begin{table}[H]
	\caption{Сранение Active learning со случайным выбором примеров}
	
	\begin{small}
		\begin{center}
			\begin{tabular}{|c | c | c |}
				\hline
				Тип слов  &  Active learning & Случайный выбор \\ \hline
				Все слова & 0.976          & 0.960            \\ \hline
				Омографы  & 0.883          & 0.863      \\ \hline
			\end{tabular}
		\end{center}
	\end{small}
	\label{table:al}
\end{table}

Качество при использованиии Active leaning получилось выше, чем если обучать модель на случайных 20\% обучаюющих данных. Благодаря особому методу отбора примеров, нам удалось добиться более высокого качества на меньшем объеме данных. 



\newpage
\chapter*{Заключение}
\addcontentsline{toc}{chapter}{Заключение}
\section*{Итоги работы}
\addcontentsline{toc}{section}{Итоги работы}
В данной работе мы подробно исследовали применениие нейросетевого подхода для задачи расстановки ударений в русском языке.  Основными итогами нашей работы являются:
\begin{enumerate}[  1{)}  ]
	\item Проведено сравнение несколько нейросетевых архитектур моделей для расстановки ударений в русском языке. Лучшей архитектурой получилась модель с attention.
	\item Улучшено качество расстановки ударений с 0.979 до 0.985.
	\item Исследовано влияние представлений входных данных накачество работы модели.
	\item Исследовано влияние контекста на качество расстановки ударений.
	\item Определен эффективный размер обучающей выборке при помощии Active learning и он составил 20\% от всех обучающей выборки. 

\end{enumerate}
\section*{Дальнейшие исследования}
\addcontentsline{toc}{section}{Дальнейшие исследования}
Идеи для дальнейших исследований можно разделить на несколько групп:
\begin{enumerate}[ 1{)} ]
	\item \textbf{Архитектура модели.} Использованная нами финальная модель с attention является попыткой адаптировать универсальную модель. Использование более продвинутой универсальной модели или построение специализированной может обеспечить повышение качества. Также можно добавить модели на вход дополнительную информацию, например, морфологические признаки слов.
	\item \textbf{Работа с омографами.}  Текущая модель хорошо работает с омографами, которые являются разными частями речи или разными формами одного слова. Хотелось бы добавить обработку омографов, которые являются одной и той же частью речи и при этом имеют одну и ту же форму.
	\item \textbf{Работа с именованными сущностями.} Наша  модель плохо справляется с расстановкой ударений в именованных сущностях, из которых можно отдельно выделить группу русских фамилий. Для этих слов, например, можно построить отдельную модель.
	
\end{enumerate}

\newpage
\addcontentsline{toc}{chapter}{Список использованных источников}
\bibliography{references}

%\chapter{Tmp}
%\section{Основные обозначения и определения}

%Все говорят: Кремль, Кремль. Ото всех я слышал про него, а сам ни разу не видел. Сколько раз уже (тысячу раз), напившись или с похмелюги, проходил по Москве с севера на юг, с запада на восток, из конца в конец, насквозь и как попало – и ни разу не видел Кремля \cite{erofeev}.

%\begin{definition}\label{def:schizo}
%Шизофазия (речевая разорванность) - симптом психических расстройств, выражающийся в нарушении структуры речи, при которой, в отличие от речевой бессвязности (потока несвязанных слов), фразы строятся правильно, однако не несут никакой смысловой нагрузки, а содержание речи соответствует содержанию бреда.\cite{schizo}
%\end{definition}
%$$E = m c^2$$

%\begin{notation} 
%\begin{itemize}
%\item $H$ --- водород.
%\item $O$ --- кислород.
%\item $C$ --- углерод.
%\item $\ldots$
%\end{itemize}
%\end{notation}

%На формулки тоже можно ссылаться.

%\begin{equation}\label{eq:women}
%Women = Evil
%\end{equation}

%Согласно (\ref{eq:women}), женщины --- зло.

%Собсно, на все леммы, теоремы, примеры, замечания и тэдэ можно ссылаться.

%\begin{example}\label{ex:ivanova}
%Вот, например, Лёшка хотел Отл(10), а Иванова ему 9 поставила.
%\end{example}

%\begin{remark}\label{rem:gertsen}
%Родился на улице Герцена, в гастрономе номер двадцать два. Известный экономист, по призванию своему — библиотекарь. В народе — колхозник. В магазине — продавец. В экономике, так сказать, необходим. Это, так сказать, система… э-э-э… в составе ста двадцати единиц. Фотографируете Мурманский полуостров и получаете «Те-ле-фун-кен». И бухгалтер работает по другой линии — по линии библиотекаря. Потому что не воздух будет, академик будет! Ну вот можно сфотографировать Мурманский полуостров. Можно стать воздушным асом. Можно стать воздушной планетой. И будешь уверен, что эту планету примут по учебнику. Значит, на пользу физике пойдёт одна планета. \cite{schizo}
%\end{remark}

%В силу \cref{ex:ivanova} и \cref{rem:gertsen}, динозавры вымерли.


%\section{Ещё какая-то муть}
%Так. Стакан зубровки. А потом – на Каляевской – другой стакан, только уже не зубровки, а кориандровой. Один мой знакомый говорил, что кориандровая действует на человека антигуманно, то есть, укрепляя все члены, ослабляет душу. Со мной почему-то случилось наоборот, то есть душа в высшей степени окрепла, а члены ослабели, но я согласен, что и это антигуманно. Поэтому там же, на Каляевской, я добавил еще две кружки жигулевского пива и из горлышка альб-де-дессерт.\cite{erofeev}



%\chapter{Заметки о женской логике}
%\section{Как говорил Бек}
%В наш век точное познание завоевывает все новые области. Одна из таких областей – женская логика. Строгое изложение находится еще в стадии зарождения. Обычная мужская логика прошла эту стадию более двух тысяч лет назад, но женская логика еще ждет своего Аристотеля. Потомкам принадлежит большая и почетная задача создать систематический курс женской логики, выполнить ее аксиоматизацию, создать вычислительные машины, действующие по женским логическим схемам.\cite{bek}

%\begin{figure}[H]
	%\begin{center}
		% \includegraphics[width=0.5\linewidth]{Baseline}
%	\end{center}
  
   % \caption{\footnotesize{Чёрный квадрат}}
%\label{fig:square}
%\end{figure}

%На картинку тоже можно ссылаться: \cref{fig:square}

%\section{Теорема Сосницкого}
%Следующая теорема даёт ответы на все ваши вопросы.

%\begin{theorem}[Теорема Сосницкого]\label{th:sosnitsky}
%Lorem ipsum dolor sit amet, consectetur adipiscing elit.
%\end{theorem}

%Для доказательства теоремы понадобится следующая лемма:

%\begin{lemma}\label{le:sosnyakovsky}
%Если в кране нет воды, значит выпили жиды.
%\end{lemma}
%\begin{proof}
%Если в кране есть вода, значит жид нассал туда.

%\end{proof}

%\begin{enumerate}
%\item Пер.
%\item Пер.
%\item Пер.
%\end{enumerate}

%\begin{corollary}\label{cor:erofeev}
%О, тщета! О, эфемерность! 
%\end{corollary}
%\begin{proof}
%О, самое бессильное и позорное время в жизни моего народа – время от рассвета до открытия магазинов! Сколько лишних седин оно вплело во всех нас, в бездомных и тоскующих шатенов!
%\end{proof}

%\chapter{Как рисовать всякие красивости}

%\section{Система нумерованных уравнений}
%\begin{align}
%\label{eq:adin} \frac{dq}{dt} &= \frac{dH}{dp} \\
%\label{eq:dva} \frac{dp}{dt} &= -\frac{dH}{dq}
%\end{align}
%А всё для того, чтобы сослаться раз (\ref{eq:adin}), сослаться два (\ref{eq:dva})
%\begin{table}[H]\label{table:rokk_ebol}
%\begin{small}
%\begin{center}
%\begin{tabular}{|c|c|c|c|}
%\hline
%Р & О & К & К\\
%\hline
%Е & Б & О & Л\\
%\hline
%М & У & П & Ю\\
%\hline
%О & В & И & Ч\\
%\hline
%Н & И & R & Е\\
%\hline
%Т &   &   & Й\\
% абсв & & & \\
%\hline
%\end{tabular}
%\end{center}
%\end{small}
%\caption{ Sample text.}

%\end{table}

%Что бы вы думали можно сделать с \cref{table:rokk_ebol}

%\section{Дерево}
%Хер знает зачем, но вдруг пригодится.

%\begin{figure}[H]
%\Tree[.(start) [.РОКК  [.р [.о [.к ] ] ]
   %                     [.к [.е [.б ] ]
      %                          [.о [.л ] ] ] ]
         %      [.ЕБОЛ  [.Lorem [.Ipsum [.Dolor ] ] ]
            %            [.Set [.Amet [.fgs ] ]
               %                 [.fds [.foobar ] ] ] ]
%               [.МУПЮ ]
   %            [.ОВИЧ [.раз [.раз [.раз ] ] ]
      %                     [.это [.хард [.басс ] ]
         %                              [.саня [.колбасёр ] ] ] ] ]
%    \caption{\footnotesize{Смотри как умею}}
%\label{fig:tree}
%\end{figure}
%Опа \cref{fig:tree}.

\end{document}
